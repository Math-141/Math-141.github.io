---
pagetitle: "Random Variables II"
editor: source
format: 
  revealjs:
    chalkboard: true
    incremental: true
    theme: [default, custom.scss]
    height: 900
    width: 1600
    slide-number: c
    auto-stretch: false
    callout-appearance: simple
    pdf-max-pages-per-slide: 2
    menu: 
      side: right
      numbers: true
execute:
  echo: false
  warning: false
  message: false
---

```{r}
#| include: false
#| warning: false
#| message: false

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = 'center')
library(knitr)
library(tidyverse)
library(ggthemes)
library(moderndive)
library(gapminder)
library(infer)
theme_set(theme_bw())
```

::::: columns
::: {.column .center width="60%"}
![](img/DAW.jpeg){width="90%"}
:::

::: {.column .center width="40%"}
<br>

[Random Variables II]{.custom-title}

<br> <br> <br> <br> <br>

[Grayson White]{.custom-subtitle}

[Math 141 <br> Week 10 \| Fall 2025]{.custom-subtitle}
:::
:::::

------------------------------------------------------------------------

## Goals for Today

- Continue our discussion of expected value and variance
- Introduce the **Bernoulli distribution**
- Introduce the **Binomial distribution**


# Review


## Review: Expected Value, Variance, and Standard Deviation

The **expected value** (or mean) of a discrete random variable $X$ is
$$
\mu = E[X] = \sum_{i =1}^n x_i P(X = x_i)
$$
where $x_1, \dots, x_n$ are *all the values X could potentially take*.

::: {.fragment}
The **variance** and **standard deviation** of a discrete random variable $X$ with mean $E(X) = \mu$ is
$$
\sigma^2 = \mathrm{Var}(X) = \sum_{i =1}^n (x_i-\mu)^2 P(X = x_i)
$$
$$
\sigma = \textrm{SD}(X) = \sqrt{\mathrm{Var}(X)}
$$
:::

------------------------------------------------------------------------

## Review: Rules for Expected Value and Variance

:::: {.columns}


::: {.column width=50%}

::: {.callout-note icon=false}
## Theorem: Expectation of Sum
Let $X$ and $Y$ be random variables. Then
$$
E(X + Y) = E(X) + E(Y)
$$
:::

::: {.callout-note icon=false}
## Theorem: Scalar Multiplication with Expectation
Let $X$ be a random variable, and let $c$ be a number. Then
$$
E(cX) = cE(X) 
$$
:::

:::

::: {.column width=50%}



::: {.callout-note icon=false}
## Theorem: Variance of Sum
Let $X$ and $Y$ be random variables. Additionally, let $X$ and $Y$ be *independent*. Then
$$
\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)
$$
:::


::: {.callout-note icon=false}
## Theorem: Scalar Multiplication with Variance
Let $X$ be a random variable, and let $c$ be a number. Then
$$
\mathrm{Var}(cX) = c^2\mathrm{Var}(X) 
$$
:::

:::

::::

------------------------------------------------------------------------

## Recall: One Coin Flip

Consider a random variable $X$ which is the number of heads in a single coin flip.

::: {.fragment}
**Q:** What are the possible values for $X$? What are the probabilities each value of $X$?
:::

::: {.fragment}
::: {.nonincremental}
- $X = 1$ (H) with $P(X=1) = \frac{1}{2}$
- $X = 0$ (T) with $P(X=0) = \frac{1}{2}$
:::
:::

::: {.fragment}
**Q:** Compute the expected value and variance for $X$ (the number of heads in a single coin flip)
:::

::: {.fragment}
$$
E[X] = (0)\frac{1}{2} +  (1)\frac{1}{2} = \frac{1}{2}
$$
:::

::: {.fragment}
$$
Var[X] = (0-1/2)^2\frac{1}{2} +  (1-1/2)^2\frac{1}{2} = \frac{1}{4}
$$
:::

::: {.fragment}

This distribution has a name! 

-   $X\sim \text{Bernoulli}(p = 0.5)$

:::


# Specific Named Random Variables

------------------------------------------------------------------------

### Specific Named Random Variables

-   There is a vast array of random variables out there.

-   But there are a few particular ones that we will find useful.

    -   Because these ones are used often, they have been given names.

-   Will identify these named RVs using the following format:

::: fragment
$$
X \sim \mbox{Name(values of key parameters)}
$$
:::

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
Important parameter:

$$
\begin{align*}
p & = \mbox{probability of success} \\
& = P(X = 1)
\end{align*}
$$
:::

::: column
```{r, echo = FALSE, fig.asp = 0.5}
library(tidyverse)
dat <- data.frame(y = c(.3, .7), x = c(0, 1))
ggplot(dat, aes(factor(x), y)) + 
  geom_col(fill = "deeppink3") +
  labs(title = expression(X %~% " Bernoulli(p = 0.7)"),
       x = "x", y = "p(x)")
```

Distribution:

| $x$    | 0       | 1   |
|--------|---------|-----|
| $p(x)$ | 1 - $p$ | $p$ |
:::
:::

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p = 0.5)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
:::

::: column
```{r, echo = FALSE, fig.asp = 0.5}
library(tidyverse)
dat <- data.frame(y = c(.5, .5), x = c(0, 1))
ggplot(dat, aes(factor(x), y)) + 
  geom_col(fill = "deeppink3") +
  labs(title = expression(X %~% " Bernoulli(p = 0.5)"),
       x = "x", y = "p(x)")
```

Distribution:

| $x$    | 0   | 1   |
|--------|-----|-----|
| $p(x)$ | 0.5 | 0.5 |
:::
:::

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
:::

::: column
Distribution:

| $x$    | 0       | 1   |
|--------|---------|-----|
| $p(x)$ | 1 - $p$ | $p$ |
:::
:::

Mean:

$$
\begin{align*}
\mu &= \sum x p(x) \\
& = 1*p + 0*(1 - p) \\
& = p
\end{align*}
$$

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
:::

::: column
Distribution:

| $x$    | 0       | 1   |
|--------|---------|-----|
| $p(x)$ | 1 - $p$ | $p$ |
:::
:::

Standard deviation:

$$
\begin{align*}
\sigma & = \sqrt{ \sum (x - \mu)^2 p(x)} \\
& = \sqrt{(1 - p)^2*p + (0 - p)^2*(1 - p)}  \\
& = \sqrt{p(1 - p)}
\end{align*}
$$


# What about the sum of many Bernoulli random variables?


## $n$ coin tosses: Expected Value and Variance

Let $X$ be the number of heads in $n$ independent coin tosses.

**Q:** What are $E(X)$ and $\mathrm{Var}(X)$?

::: {.fragment}
*Hint:* We know, from the Bernoulli distribution, that if $X_1$ is the number of heads in a *single* coin flip,
$$E(X_1)=0.5 \quad \text{ and } \quad \text{Var}(X_1)=0.25$$
:::


------------------------------------------------------------------------

## $n$ coin tosses: Expected Value and Variance

Let $X$ be the number of heads in $n$ independent coin tosses.


**Answer:** Let $X_1$ = Heads in 1st toss, $\dots$  , $X_n$ = Heads in $n$th toss:
$$
  X = X_1 + \dots + X_n = \sum_{i=1}^n X_i
$$



$$
\begin{align}
  E(X) &= E \left( \sum_{i=1}^n X_i \right) =  \sum_{i=1}^n E(X_i) = \sum_{i=1}^n 0.5 = 0.5n\\
  \mathrm{Var}(X) &=  \mathrm{Var}\left( \sum_{i=1}^n X_i \right) =  \sum_{i=1}^n  \mathrm{Var}(X_i) = \sum_{i=1}^n 0.25 = 0.25n
\end{align}
$$


## $n$ coin toss probabilities? {.smaller}

We've calculated Expectation and Variance for the number of heads in $n$ coin tosses.

::: {.fragment}
::: {.nonincremental}
- But what about the **probability** that there are $k$ heads in the $n$ tosses?
:::
:::

::: {.fragment}
For example, **What's the probability of $k=2$ heads in $n=3$ coin tosses?**
:::

::: {.fragment}
::: {.nonincremental}
- Let's break down the problem: **What are the possible outcomes?**
:::
:::

::: {.fragment}
$$
\begin{align}
\text{TTT} & \implies X=0 \quad& \text{TTH or THT or HTT} & \implies X=1\\
\text{HHH} & \implies X=3 \quad& \text{THH or HTH or HHT} & \implies X=2\\
\end{align}
$$
:::

::: {.fragment}
::: {.nonincremental}
- The **3 coin flips** had $8$ possible outcomes, each with equal probability.
:::
:::

::: {.fragment}
Differently, $X$ had $4$ possible outcomes: 0, 1, 2, or 3. Those probabilities are:
$$
\begin{align}
P(X=0) &= \frac{1}{8} \quad&\quad P(X=1) &= \frac{3}{8}\\
P(X=3) &= \frac{1}{8} \quad&\quad P(X=2) &= \frac{3}{8}\\
\end{align}
$$
:::

------------------------------------------------------------------------

## 3 coin tosses probabilities

Notice that the probability of $k$ heads in $n$ coin flips is

$$P[X=k] = \big(\text{Number of ways to get k heads}\big) \times \big(\text{Prob of each way}\big)$$

::: {.fragment}
For example, we found that in $n=3$ coin tosses,

- 3 ways to get 2 heads
- Each sequence of 3 coin flips has probability $\frac12\times\frac12\times\frac12=\frac18$
- Thus, $P(X=2) = \frac38$
:::

------------------------------------------------------------------------

## Generalization 1 {.smaller}

First: **What if $n$ and $k$ are bigger?**

::: {.fragment}
- In this case, it's hard to count all the different combinations.
:::

::: {.fragment}
- Instead, we can rely on a **handy formula**: The number of ways to get $k$ "successes" in $n$ "trials" is
$${n\choose k} = \frac{n!}{k!(n-k)!}$$
We read this as **"n choose k"**
:::

::: {.fragment}
The exclamation point is a **factorial**:
$$
\begin{align}
  0! &= 1\\
  1! &= 1\\
  2! &= 2\times 1=2\\
  3! &= 3\times2\times1=6\\
  4! &= 4\times3\times2\times1=24
\end{align}
$$
:::

------------------------------------------------------------------------

## Generalization 2

Second: **What if the probability of "success" changes?**

::: {.fragment}
- Before, we were flipping fair coins, i.e., $p=0.5$.
- But we could think of flipping "unfair coins", i.e., $p$ is a number between 0 and 1.
- In other words, how can we generalize to the sum of Bernoulli($p$) case from the Bernoulli($p=0.5$) case?
:::

::: {.fragment}
- In this case, the probability of getting *a sequence* of $k$ "successes" in $n$ trials is:
$$p^k\times(1-p)^{n-k}$$
:::

::: {.fragment}
- There are $k$ "successes", each with probability $p$: $p^k$
- There are $n-k$ "failures", each with probability $1-p$: $(1-p)^{n-k}$
:::

------------------------------------------------------------------------

## Putting it all together

Thus, the probability of observing $k$ successes in $n$ independent Bernoulli trials with probability of success $p$ is

$$P[X=k] = {n \choose k}p^k(1-p)^{n-k}$$

In this case, $X \sim \text{Binomial}(n,~p)$. Or, in words: $X$ follows a binomial distribution with $n$ trials each with $p$ probability of success

::: {.fragment}
**How to tell if your distribution is a Binomial?**

- Number of trials, $n$, must be **fixed**.
- Each trial must be **Bernoulli**, with success probability $p$. ($p$ must stay the same for each trial!)
- We must be counting the **number of successes**
:::


# Properties

------------------------------------------------------------------------

## Expectation and Variance of Binomial

If $X \sim \text{Binomial}(n,~p)$, then:

::: {.fragment}
$$
\begin{align}
    E(X) &= np \\
    Var(X) &= np(1-p)
\end{align}
$$
:::

::: {.fragment}
Note that when $p=0.5$ we get
$$
\begin{align}
    E(X) &= 0.5n \\ 
    Var(X) &= 0.25n
\end{align}
$$
which is what we derived earlier!
:::

------------------------------------------------------------------------

## Calculating Binomial Probabilities: `dbinom()`

If $X$ is a binomial random variable with $n$ trials and probability of success $p$, we can use `R` to calculate the probabilities with the `dbinom()` function

::: {.fragment .nonincremental}
- e.g., $P(X = 2)$ where $n=3$ and $p=0.5$ (2 heads in 3 fair coin tosses)
```{r}
#| echo: true
dbinom(x=2, size=3, prob=0.5)
```
:::

::: {.fragment .nonincremental}
- e.g., $P(X = 10)$ where $n=15$ and $p=0.7$
```{r}
#| echo: true
dbinom(x=10, size=15, prob=0.7)
```
:::

------------------------------------------------------------------------

## Calculating Binomial Probabilities: `pbinom()`

We can also calculate $P(X \leq k)$ with `pbinom()`

::: {.fragment .nonincremental}
- e.g., $P(X \leq 2)$ where $n=3$ and $p=0.5$ (2 or fewer heads in 3 fair coin tosses)
```{r}
#| echo: true
pbinom(q=2, size=3, prob=0.5)
```
:::

::: {.fragment}
Can also calculate $P(X > k)$ by doing `1 - pbinom()`:
:::

::: {.fragment .nonincremental}
- e.g., $P(X > 2)$ where $n=3$ and $p=0.5$ (more than 2 heads in 3 fair coin tosses)
```{r}
#| echo: true
1 - pbinom(q=2, size=3, prob=0.5)
```
:::

::: {.fragment .nonincremental}
- e.g., $P(X \geq 2)$ where $n=3$ and $p=0.5$
```{r}
#| echo: true
1 - pbinom(q=1, size=3, prob=0.5)
```
:::

------------------------------------------------------------------------

## Activity: Survey of Portlanders

Suppose you believe 40% of Portlanders think the city should install more bike lanes. You will take a simple random survey of 50 Portlanders to test your belief.

1. Assume your belief is true. Let $X$ be the number of survey respondents who think the city should install more bike lanes. Does $X$ follow a Binomial distribution? If so, what are $n$ and $p$?

2. Calculate the expected value and standard deviation of $X$ (still assuming your belief is true)

3. Suppose you conducted the survey and found 30 respondents wanted more bike lanes. What's the probability that $X\geq 30$? (use R!)

4. Draw the connection between the probability you calculated and a hypothesis test.

```{r}
#| echo: false
countdown::countdown(8)
```


------------------------------------------------------------------------

## Activity: Survey of Portlanders (Answers) {.smaller}

Suppose you believe 40% of Portlanders think the city should install more bike lanes. You will take a simple random survey of 50 Portlanders to test your belief.

::: {.fragment .nonincremental}
1. Assume your belief is true. Let $X$ be the number of survey respondents who think the city should install more bike lanes. Does $X$ follow a Binomial distribution? If so, what are $n$ and $p$?

    - $X$ follows a Binomial($n=50,p=0.4$) distribution.

:::

::: {.fragment .nonincremental}
2. Calculate the expected value and standard deviation of $X$ (assuming your belief)


    - $E[X] = (50)(.4) = 20$
    - $SD[X] = \sqrt{(50)(.4)(1-.4)} = 3.46$

:::

::: {.fragment .nonincremental}
3. Suppose you conducted the survey and found 30 respondents wanted more bike lanes. What's the probability that $X\geq 30$? (use R!)

    - $P[X\geq30] = 1 - \texttt{pbinom}(29,~ 50,~ .4) = 0.0033$ 

:::

::: {.fragment .nonincremental}
4. Draw the connection between the probability you calculated and a hypothesis test.

    - We are testing $H_0: p=0.4$ vs. $H_a: p>0.4$
    - Test statistic: $\hat{p}=30/50$
    - p-value = 0.0033

:::