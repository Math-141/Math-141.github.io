---
pagetitle: "SLR II: Regression Assumptions"
editor: source
format: 
  revealjs:
    chalkboard: true
    incremental: true
    theme: [default, custom.scss]
    height: 900
    width: 1600
    slide-number: c
    auto-stretch: false
    callout-appearance: simple
    pdf-max-pages-per-slide: 2
    menu: 
      side: right
      numbers: true
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| include: false
#| warning: false
#| message: false

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = 'center')
library(knitr)
library(tidyverse)
library(cowplot)
```

::::: columns
::: {.column .center width="60%"}
![](img/DAW.jpeg){width="90%"}
:::

::: {.column .center width="40%"}
<br>

[SLR II: Regression Assumptions]{.custom-title}

<br> <br> 

[Grayson White]{.custom-subtitle}

[Math 141 <br> Week 4 \| Fall 2025]{.custom-subtitle}
:::
:::::

------------------------------------------------------------------------

### Announcements

-   Lab tomorrow meets in the **thesis tower**. 

::: {.fragment .nonincremental}

-   Fill out the [Week 4 Survey](https://forms.gle/NBsSe2A7n4zyCvVE9)

![](img/adobe-express-qr-code.png){width=20%}

:::

### Goals for Today

-   Recall simple linear regression
-   Do simple linear regression in R
-   Discuss model assumptions for linear regression

------------------------------------------------------------------------

## Form of the Model

<br><br><br>

$$
y = f(x) + \epsilon
$$

<br><br><br>

**Goal:**

-   Determine a reasonable form for $f()$. (Ex: Line, curve, ...)

-   Estimate $f()$ with $\hat{f}()$ using the data.

-   Generate predicted values: $\hat y = \hat{f}(x)$.

------------------------------------------------------------------------


### Simple Linear Regression Model

$$
y = \beta_o + \beta_1 x + \epsilon
$$

Consider this model when:

-   Response variable $(y)$: quantitative

-   Explanatory variable $(x)$: quantitative

    -   Have only ONE explanatory variable.

-   AND, $f()$ can be approximated by a line.

-   Need to determine the best **estimates** of $\beta_o$ and $\beta_1$.

------------------------------------------------------------------------

#### Distinguishing between the **population** and the **sample**

::::: columns
::: column
$$ 
y = \beta_o + \beta_1 x + \epsilon
$$

-   Parameters:
    -   Based on the **population**
    -   Unknown then if don't have data on the whole population
    -   EX: $\beta_o$ and $\beta_1$
:::

::: column
$$
\hat{y} = \hat{ \beta}_o + \hat{\beta}_1 x
$$

-   Statistics:
    -   Based on the **sample** data
    -   Known
    -   Usually estimate a population parameter
    -   EX: $\hat{\beta}_o$ and $\hat{\beta}_1$
:::
:::::

------------------------------------------------------------------------


### Our Modeling Goal

Recall our modeling goal: **predict win percentage by using the price percentage variable**.

```{r}
#| output-location: column
candy <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/candy-power-ranking/candy-data.csv") %>%
  mutate(pricepercent = pricepercent*100)

ggplot(data = candy, 
       mapping = aes(x = pricepercent,
                     y = winpercent)) +
  geom_point(alpha = 0.6, size = 4, 
             color = "chocolate4") +
  geom_smooth(method = "lm", se = FALSE,
              color = "deeppink2")
```

------------------------------------------------------------------------

### Method of Least Squares

:::::: columns
:::: column
-   Want residuals to be small.

-   Minimize a function of the residuals.

-   Minimize:

::: fragment
$$
\sum_{i = 1}^n e^2_i
$$
:::
::::

::: column
```{r, echo=FALSE, eval = TRUE}
ggplot(data = candy, 
       mapping = aes(x = pricepercent,
                     y = winpercent)) +
  geom_point(alpha = 0.6, size = 4, 
             color = "chocolate4") +
  stat_smooth(method = "lm", se = FALSE,
              color = "deeppink2") +
  annotate("segment", x = candy$pricepercent, 
           xend = candy$pricepercent, 
           y = candy$winpercent, yend = 42 + .178*candy$pricepercent, 
           color = "darkblue", arrow = arrow(length = unit(0.03, "npc"))) + 
  geom_point()
  
```
:::
::::::

------------------------------------------------------------------------

### Method of Least Squares

After minimizing the sum of squared residuals, you get the following equations:

Get the following equations:

$$ 
\begin{align}
\hat{\beta}_1 &= \frac{ \sum_{i = 1}^n (x_i - \bar{x}) (y_i - \bar{y})}{ \sum_{i = 1}^n (x_i - \bar{x})^2} \\
\hat{\beta}_o &= \bar{y} - \hat{\beta}_1 \bar{x}
\end{align}
$$ where

$$
\begin{align}
\bar{y} = \frac{1}{n} \sum_{i = 1}^n y_i \quad \mbox{and} \quad \bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i
\end{align}
$$

------------------------------------------------------------------------

### Method of Least Squares

Then we can estimate the whole function with:

$$
\hat{y} = \hat{\beta}_o + \hat{\beta}_1 x
$$

Called the **least squares line** or the **line of best fit**.

------------------------------------------------------------------------

### Constructing the Simple Linear Regression Model in R

We can use the `lm()` function to construct the simple linear regression model in R. 

::: {.fragment}

```{r, R.options=list(pillar.sigfig = 5)}
mod <- lm(winpercent ~ pricepercent, data = candy)

library(moderndive)
get_regression_table(mod)
```

:::

:::{.fragment}
What is the fitted model form? 
:::

:::{.fragment}
\begin{align*}
  \hat{y} &= \hat{\beta_o} + \hat{\beta_1} \times x_{pricepercent} \\
   &= 41.979 + 0.178 \times x_{pricepercent}
\end{align*}
::: 

:::{.fragment}
How do we interpret the coefficients?
:::

------------------------------------------------------------------------

### Coefficient Interpretation

\begin{align*}
  \hat{y} &= \hat{\beta}_o + \hat{\beta}_1 \times x_{pricepercent} \\
   &= 41.979 + 0.178 \times x_{pricepercent}
\end{align*}

We need to be precise and careful when interpreting estimated coefficients! 
  
-   **Intercept:** We [expect/predict]{.underline} $y$ to be $\hat{\beta}_o$ [on average]{.underline} when $x = 0$.

-   **Slope:** For a one-unit increase in $x$, we [expect/predict]{.underline} $y$ to change by $\hat{\beta}_1$ units [on average]{.underline}.

:::{.fragment}

**These interpretations are non-specific to the context of our model, but when we are interpreting coefficients, we always need to interpret the coefficients in context**

:::

------------------------------------------------------------------------

### Coefficient Interpretation: In Context

\begin{align*}
  \hat{y} &= \hat{\beta}_o + \hat{\beta}_1 \times x_{pricepercent} \\
   &= 41.979 + 0.178 \times x_{pricepercent}
\end{align*}

-   **Intercept:** We [expect/predict]{.underline} a candy's win percentage to be 41.979 [on average]{.underline} when their price percentage is 0.

-   **Slope:** For a one-unit increase in price percentage, we [expect/predict]{.underline} the win percentage of a candy to change by 0.178 units [on average]{.underline}.



------------------------------------------------------------------------

### Prediction

```{r}
new_cases <- data.frame(pricepercent = c(25, 85, 150))
predict(mod, newdata = new_cases)
```

-   We didn't have any treats in our sample with a price percentage of 85%. Can we still make this prediction?
    -   Called **interpolation**

<br>

-   We didn't have any treats in our sample with a price percentage of 150%. Can we still make this prediction?
    -   Called **extrapolation**

------------------------------------------------------------------------

### Cautions

::::: columns
::: column
-   Careful to only predict values within the range of $x$ values in the sample.

-   Make sure to investigate **outliers**: observations that fall far from the cloud of points.
    - High leverage points
:::

::: column
```{r, echo = FALSE}
set.seed(13681)
x <- c(runif(50, 0, 10))
y1 <- c(3 + 1*x + rnorm(50, 0))
dat1 <- data_frame(x = c(x, 20), y = c(y1, 20))
x2 <- c(x, 20)
y2 <- c(y1, 5)
dat <- data_frame(x = x2, y = y2) 
p1 <- ggplot(dat, aes(x, y)) + geom_point() + 
  stat_smooth(method = "lm", se = FALSE, color = "turquoise4", linewidth = 2) +
  stat_smooth(method = "lm", se = FALSE, data = dat[-51,], color = "deeppink3", linewidth = 2)
p2 <- ggplot(dat1, aes(x, y)) + geom_point() + 
  stat_smooth(method = "lm", se = FALSE, color = "turquoise4", linewidth = 2) +
  stat_smooth(method = "lm", se = FALSE, data = dat[-51,], color = "deeppink3", linewidth = 2)
plot_grid(p1, p2, ncol = 2)
```
:::
:::::


------------------------------------------------------------------------

### A closer look at our model

```{r, R.options=list(pillar.sigfig = 5)}
get_regression_table(mod)
```


\begin{align*}
  \hat{y} &= \hat{\beta_o} + \hat{\beta_1} \times x_{pricepercent} \\
   &= 41.979 + 0.178 \times x_{pricepercent}
\end{align*}


:::{.fragment}
What assumptions have we made?
:::

## Linear Regression Assumptions {.smaller}

We can *always* find the line of best fit to explore data, but...


:::{.fragment}
**To make accurate predictions or inferences, certain conditions should be met.**
:::

:::{.fragment}
To responsibly use linear regression tools for prediction or inference, we require:
:::

:::{.fragment .nonincremental}
1. **Linearity:** The relationship between explanatory and response variables must be approximately linear
    
    - Check using scatterplot of data, or residual plot
::: 

:::{.fragment .nonincremental}
2. **Independence:** The observations should be independent of one another.

    - Can check by considering data context, and
    - by looking at residual scatterplots too
::: 

:::{.fragment .nonincremental}
3. **Normality:** The distribution of residuals should be *approximately* bell-shaped, unimodal, symmetric, and centered at 0 at every "slice" of the explanatory variable

    - Simple check: look at histogram of residuals
    - Better to use a "Q-Q plot" 
:::  

:::{.fragment .nonincremental}
4. **Equal Variability:** Variance of residuals should be roughly constant across data set. Also called "homoscedasticity". Models that violate this assumption are sometimes called "heteroscedastic"
    
    - Check using residual plot.
:::  

  
## A cute way to remember this: **"LINE"**  


1. **L**inearity
    
2. **I**ndependence

3. **N**ormality
    
4. **E**qual Variability


## Assessing conditions: diagnostics plots

::::{.columns}

::: {.column width=50%}

In order to assess if we've met the described conditions, we will utilize four common **diagnostics plots**: 

-   Residual vs. fitted plot (i.e. 'residual plot')
    - for assessing linearity, independence, and equal variability
-   Normal Q-Q plot
    - for assessing normality
-   Residual histogram
    - for assessing normality
-   Residual vs. leverage plot
    - for assessing the influence of outliers 
    
:::

:::{.column width=5%}

:::

:::{.column width=45%}

:::{.fragment}
    
In order to make these plots, we'll use the `gglm` (grammer of graphics for linear model diagnostics) package.

![](https://raw.githubusercontent.com/graysonwhite/gglm/master/figs/gglm.gif){width=50% fig-align='center'}


```{r}
# Run the below code (without the '#') once
# install.packages("gglm") 
# Then load with:
library(gglm)
```

:::

:::

::::

## Recall out simple linear regression model

```{r}
mod <- lm(winpercent ~ pricepercent, data = candy)
```


```{r}
#| echo: false
ggplot(data = candy, 
       mapping = aes(x = pricepercent,
                     y = winpercent)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "deeppink2")
```

:::{.fragment}
Let's check if this meets the **LINE** assumptions
:::


## Residual vs. fitted plot

```{r}
ggplot(mod) + 
  stat_fitted_resid()
```

-   Linearity: `r emo::ji("check")`

-   Independence: `r emo::ji("check")`

-   Equal Variability: `r emo::ji("check")`


## Checking normality: residual histogram


```{r}
ggplot(mod) + 
  stat_resid_hist(bins = 15)
```

-   Normality: `r emo::ji("thinking")`
    - Looks pretty good! 
    - We'll look at a Q-Q plot to further assess!

## Checking normality: Q-Q plot

```{r}
ggplot(mod) + 
  stat_normal_qq()
```

-   Normality: `r emo::ji("check")`



## Checking outliers: Residuals vs. leverage plot

```{r}
ggplot(mod) + 
  stat_resid_leverage()
```

-   Looks pretty good! `r emo::ji("star_struck")`

# Now let's look at some models that violate these assumptions...

## New model

Predicting miles per gallon from engine displacement

```{r}
data(mtcars)
ggplot(mtcars, aes(x = disp, y = mpg)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

## New model

```{r}
mpg_mod <- lm(formula = mpg ~ disp, data = mtcars)

get_regression_points(mpg_mod)
```


## Diagnostics: residual vs. fitted plot

```{r}
ggplot(mpg_mod) + 
  stat_fitted_resid()
```

-   Linearity: `r emo::ji("x")`

-   Independence: `r emo::ji("x")`

-   Equal Variability: `r emo::ji("question")`

## Checking normality

:::: {.columns}

::: {.column width=50%}

```{r}
ggplot(mpg_mod) + 
  stat_resid_hist(bins = 10)
```

:::


::: {.column width=50%}

```{r}
ggplot(mpg_mod) + 
  stat_normal_qq(alpha = 1)
```

:::

::::

-   Normality: Not horrible, but surely could be better!

## Another example: high leverage points

Remember the example from last time:

```{r}
#| echo: false
set.seed(13681)
x <- c(runif(50, 0, 10))
y1 <- c(3 + 1*x + rnorm(50, 0))
dat1 <- data_frame(x = c(x, 20), y = c(y1, 20))
x2 <- c(x, 20)
y2 <- c(y1, 5)
dat <- data_frame(x = x2, y = y2) 
ggplot(dat, aes(x, y)) + geom_point() + 
  stat_smooth(method = "lm", se = FALSE, color = "turquoise4", linewidth = 2) +
 stat_smooth(method = "lm", se = FALSE, data = dat[-51,], color = "deeppink3", linewidth = 2)
```

:::{.fragment}
What do diagnostics look like when we fit the teal model?
:::

## Diagnosing the model

```{r}
teal_mod <- lm(y ~ x, dat)
ggplot(teal_mod) + 
  stat_fitted_resid()
```

-   In this case, can already see the outlier in the residual vs fitted plot. 


## Residual vs. leverage

```{r}
teal_mod <- lm(y ~ x, dat)
ggplot(teal_mod) + 
  stat_resid_leverage()
```

## What to do with high leverage points? 

-   Depends on **context**
-   It may be reasonable to remove them and refit the model


::: {.fragment}

```{r}
dat_no_outlier <- dat %>%
  filter(x < 15)

pink_mod <- lm(y ~ x, dat_no_outlier)
```

:::


## A shortcut for diagnosing: `gglm::gglm()`

You can see many of the diagnostic plots at once with the `gglm()` function from the `gglm` package. 

Let's use that function to diagnose the pink model

:::{.fragment}

```{r}
gglm(pink_mod)
```

:::


