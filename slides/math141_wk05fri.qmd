---
pagetitle: "Model Guidance"
editor: source
format: 
  revealjs:
    chalkboard: true
    incremental: true
    theme: [default, custom.scss]
    height: 900
    width: 1600
    slide-number: c
    auto-stretch: false
    callout-appearance: simple
    pdf-max-pages-per-slide: 2
    menu: 
      side: right
      numbers: true
execute:
  echo: true
  warning: false
  message: false
---

```{r}
#| include: false
#| warning: false
#| message: false

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = 'center')
library(knitr)
library(tidyverse)
```

::::: columns
::: {.column .center width="60%"}
![](img/DAW.jpeg){width="90%"}
:::

::: {.column .center width="40%"}
<br>

[Model Guidance]{.custom-title}

<br> <br> 

[Grayson White]{.custom-subtitle}

[Math 141 <br> Week 5 \| Fall 2025]{.custom-subtitle}
:::
:::::

------------------------------------------------------------------------


### Goals for Today

-   Finish up: Regression with polynomial explanatory variables
-   Modeling guidance
    - Introduce two measures of model fit: $R^2$ and adjusted $R^2$




------------------------------------------------------------------------

### Which Are You?

:::::: columns
::: {.column width="33%"}
Data Visualizer

<iframe src="https://giphy.com/embed/d31vTpVi1LAcDvdm" width="480" height="362" frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>
:::

::: {.column width="33%"}
Data Wrangler

<iframe src="https://giphy.com/embed/DbaUtl1DcLyrdwhzGJ" width="480" height="362" frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>
:::

::: {.column width="33%"}
Model Builder

<br>

<iframe src="https://giphy.com/embed/xZsLh7B3KMMyUptD9D" width="480" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>

</iframe>
:::
::::::

------------------------------------------------------------------------

### Linear Regression

Model Form:

$$ 
\begin{align}
y &= \beta_o + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\end{align}
$$

Linear regression is a flexible class of models that allow for:

::: nonincremental
-   Both quantitative and categorical **explanatory** variables.

-   **Multiple** explanatory variables.

-   **Curved** relationships between the response variable and the explanatory variable.

-   BUT the **response variable is quantitative**.
:::

------------------------------------------------------------------------

### Linear Regression & Curved Relationships

**Form of the Model:**

$$ 
\begin{align}
y &= \beta_o + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\end{align}
$$

But why is it called **linear** regression if the model also handles for curved relationship??

# Which model is best?

------------------------------------------------------------------------

### Model Building Guidance

What degree of polynomial should I include in my model?

:::::: fragment
::::: columns
::: column
<br>

**Guiding Principle**: Capture the general trend, not the noise.

$$
\begin{align}
y &= f(x) + \epsilon \\
y &= \mbox{TREND} + \mbox{NOISE}
\end{align}
$$
:::

::: column
Returning the 2008 Election Example:

```{r, echo = FALSE}
library(Stat2Data)
data("Pollster08")
ggplot(Pollster08, aes(x = Days, y = Margin)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, degree = 2), 
              color = "purple") +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, degree = 5), 
              color = "orange") +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, degree = 18), 
              color = "darkgreen")
```
:::
:::::
::::::

------------------------------------------------------------------------

## Model Building Guidance

Shouldn't we always include the interaction term?

:::::: columns
:::: column
**Guiding Principle**: Occam's Razor for Modeling

> "All other things being equal, simpler models are to be preferred over complex ones." -- ModernDive

**Guiding Principle**: Consider your modeling goals.

::: nonincremental
-   The equal slopes model allows us to control for the intensity of the light and then see the impact of being in the early or late timing groups on the number of flowers.
:::
::::

::: column
```{r mf, echo = FALSE}
library(Sleuth3)
data(case0901)

# Recode the timing variable
case0901 <- case0901 %>%
  mutate(TimeCat = factor(case_when(
    Time == 1 ~ "Late",
    Time == 2 ~ "Early"
  )))

ggplot(case0901, aes(x = Intensity, y = Flowers,
                     color = TimeCat)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```

-   Later in the course will learn statistical procedures for determining whether or not a particular term should be included in the model.
:::
::::::

# What if I want to include many explanatory variables??

------------------------------------------------------------------------

### Model Building Guidance

We often have several potential explanatory variables. How do we determine which to include in the model and in what form?

**Guiding Principle**: Include explanatory variables that attempt to explain **different** aspects of the variation in the response variable.

------------------------------------------------------------------------

### Example: Movie Ratings

```{r}
library(tidyverse)
library(moderndive)
movies <- read_csv("https://www.lock5stat.com/datasets2e/HollywoodMovies.csv")

# Restrict our attention to dramas, horrors, and actions
movies2 <- movies %>%
  filter(Genre %in% c("Drama", "Horror", "Action")) %>%
  drop_na(Genre, AudienceScore, RottenTomatoes)
glimpse(movies2)
```

------------------------------------------------------------------------

### Example: Movie Ratings

```{r}
library(GGally)
movies2 %>%
  select(RottenTomatoes, AudienceScore, OpeningWeekend, 
         DomesticGross, Genre) %>%
  ggpairs()
```

------------------------------------------------------------------------

### Model Building Guidance

We often have several potential explanatory variables. How do we determine which to include in the model and in what form?

**Guiding Principle**: Include explanatory variables that attempt to explain **different** aspects of the variation in the response variable.

```{r}
mod_movies <- lm(RottenTomatoes ~ AudienceScore + Genre + DomesticGross, data = movies2)
get_regression_table(mod_movies)
```

------------------------------------------------------------------------

### Model Building Guidance

Suppose I built 3 different models. **Which is best?**

```{r}
mod1 <- lm(RottenTomatoes ~ AudienceScore, data = movies2)
mod2 <- lm(RottenTomatoes ~ AudienceScore + Genre, data = movies2)
mod3 <- lm(RottenTomatoes ~ AudienceScore + Genre + DomesticGross, data = movies2)
```

-   Big question! Take **Math 243: Statistical Learning** to learn systematic model selection techniques.

-   We will explore one approach. (But there are many possible approaches!)

------------------------------------------------------------------------

### Comparing Models

Suppose I built 3 different models. **Which is best?**

-   Pick the best model based on some measure of quality.

::: fragment
**Measure of quality**: $R^2$ (Coefficient of Determination)

\begin{align*}
R^2 &= \mbox{Percent of total variation in y explained by the model}\\
&= 1- \frac{\sum (y - \hat{y})^2}{\sum (y - \bar{y})^2}
\end{align*}

**Strategy**: Compute the $R^2$ value for each model and pick the one with the highest $R^2$.
:::

------------------------------------------------------------------------

### Comparing Models with $R^2$

**Strategy**: Compute the $R^2$ value for each model and pick the one with the highest $R^2$.

```{r}
mod1 <- lm(RottenTomatoes ~ AudienceScore, data = movies2)
mod2 <- lm(RottenTomatoes ~ AudienceScore + Genre, data = movies2)
mod3 <- lm(RottenTomatoes ~ AudienceScore + Genre + DomesticGross, data = movies2)

get_regression_summaries(mod1) %>% select(r_squared)
get_regression_summaries(mod2) %>% select(r_squared)
get_regression_summaries(mod3) %>% select(r_squared)
```

------------------------------------------------------------------------

**Strategy**: Compute the $R^2$ value for each model and pick the one with the highest $R^2$.

```{r}
get_regression_summaries(mod1) %>% select(r_squared)
get_regression_summaries(mod2) %>% select(r_squared)
get_regression_summaries(mod3) %>% select(r_squared)
```

::: fragment
**Problem:** As we add predictors, the $R^2$ value will only increase.

**Guiding Principle**: Occam's Razor for Modeling

> "All other things being equal, simpler models are to be preferred over complex ones." -- ModernDive
:::

------------------------------------------------------------------------

### Comparing Models with the Adjusted $R^2$

**New Measure of quality**: Adjusted $R^2$ (Coefficient of Determination)

\begin{align*}
\mbox{adj} R^2 &= 1- \frac{\sum (y - \hat{y})^2}{\sum (y - \bar{y})^2} \left(\frac{n - 1}{n - p - 1} \right)
\end{align*}

where $p$ is the number of explanatory variables in the model.

-   Now we will penalize larger models.

-   **Strategy**: Compute the adjusted $R^2$ value for each model and pick the one with the highest adjusted $R^2$.

------------------------------------------------------------------------

**Strategy**: Compute the adjusted $R^2$ value for each model and pick the one with the highest adjusted $R^2$.

```{r}
get_regression_summaries(mod1) %>% select(r_squared, adj_r_squared)
get_regression_summaries(mod2) %>% select(r_squared, adj_r_squared)
get_regression_summaries(mod3) %>% select(r_squared, adj_r_squared)
```

------------------------------------------------------------------------

### Model Building Guidance: Diagnostic Plots

Always check your diagnostic plots to ensure the model you choose is properly specified! 

```{r}
library(gglm)
gglm(mod1)
```

------------------------------------------------------------------------

### Model Building Guidance: Diagnostic Plots

Always check your diagnostic plots to ensure the model you choose is properly specified! 

```{r}
gglm(mod2)
```

------------------------------------------------------------------------

### Model Building Guidance: Diagnostic Plots

Always check your diagnostic plots to ensure the model you choose is properly specified! 

```{r}
gglm(mod3)
```

------------------------------------------------------------------------


### Model Building Guidance

We often have several potential explanatory variables. How do we determine which to include in the model and in what form?

**Guiding Principle**: Use your modeling motivation to determine how much you weigh **interpretability** versus **prediction accuracy** when choosing the model.

```{r, fig.width=13, echo=FALSE, fig.asp = 0.4}
p1 <- ggplot(Pollster08, aes(x = Days, y = Margin, 
                       color = factor(Charlie))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) + theme(legend.position = "bottom")

p2 <- ggplot(Pollster08, aes(x = Days, y = Margin, 
                       color = factor(Meltdown))) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE) + theme(legend.position = "bottom")
p3 <- ggplot(Pollster08, aes(x = Days, y = Margin)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, degree = 2))
library(cowplot)
plot_grid(p1, p2, p3, ncol = 3)

```

------------------------------------------------------------------------

### Model Building

-   Key ideas:

    -   Determining the **response** variable and the potential **explanatory** variable(s)
    -   Writing out the **model form** and understanding what the terms represent **in context**
    -   **Building** and **visualizing** linear regression models in `R`
    -   **Validating** model assumptions with diagnostic plots
    -   **Comparing** different potential models
    
    
:::{.fragment}

### Next Week

-   Introduction to Probability
-   More practice with regression in class

:::