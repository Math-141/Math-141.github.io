---
pagetitle: "Random Variables III"
editor: source
format: 
  revealjs:
    chalkboard: true
    incremental: true
    theme: [default, custom.scss]
    height: 900
    width: 1600
    slide-number: c
    auto-stretch: false
    callout-appearance: simple
    pdf-max-pages-per-slide: 2
    menu: 
      side: right
      numbers: true
execute:
  echo: false
  warning: false
  message: false
---

```{r}
#| include: false
#| warning: false
#| message: false

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, 
                      fig.retina = 3, fig.align = 'center')
library(knitr)
library(tidyverse)
library(ggthemes)
library(moderndive)
library(gapminder)
library(infer)
theme_set(theme_bw())
```

::::: columns
::: {.column .center width="60%"}
![](img/DAW.jpeg){width="90%"}
:::

::: {.column .center width="40%"}
<br>

[Random Variables III]{.custom-title}

<br> <br> <br> <br> <br> 

[Grayson White]{.custom-subtitle}

[Math 141 <br> Week 10 \| Fall 2025]{.custom-subtitle}
:::
:::::

------------------------------------------------------------------------

## Goals for Today

- Discuss continuous random variables
- Introduce the normal distribution
- Introduce the $t$ distribution
- Discuss the **Central Limit Theorem**!


# Continuous Random Variables

## The Distribution of a Continuous Variable

If $X$ is a **continuous random variable**, it can take on any value in an **interval**.

::: {.nonincremental}
- e.g., $0\leq X\leq 10$ or $-\infty < X < \infty$
:::

::: {.fragment}
Recall: For discrete random variables, we could list the probability of each possible outcome.

- For **continuous** random variables, this won't work. There are infinite outcomes!
:::

::: {.fragment}
Instead, we represent relative chances of different possible outcomes using a **density** function $f(X)$
:::

::: {.fragment}
- $f(X) \geq 0$ for all possible $X$
- The total area under the function is $1$
- $P(a\leq X\leq b)$ is the area under $f$ between $a$ and $b$
:::

------------------------------------------------------------------------

## Density Curve

Suppose $X$ is a random variable representing the time (in seconds) it takes for a particle to experience radioactive decay, where
$$
f(x) = e^{-x} \qquad \textrm{for } x\geq 0
$$

```{r}
#| fig-height: 3.5
t <- seq(0, 4, length = 200)
p <- dexp(t, 1)
dd <- data.frame(t, p)
ggplot(dd, aes(x = t, y = p)) +
  geom_area(fill = "gray", color = "black") +
  labs(x = "X", y = "Density", 
       title = "Distribution for X (time in seconds until particle decays)")
```

::: {.fragment}
::: {.nonincremental}
- The probability that it takes between $1$ and $2$ seconds to decay is the area under the curve between $1$ and $2$. $P(1 < T < 2) =$
:::
:::

------------------------------------------------------------------------

## Density Curve

Suppose $X$ is a random variable representing the time (in seconds) it takes for a particle to experience radioactive decay, where
$$
f(x) = e^{-x} \qquad \textrm{for } x\geq 0
$$

```{r}
#| fig-height: 3.5
t <- seq(0, 4, length = 200)
p <- dexp(t, 1)
dd <- data.frame(t, p)
ggplot(dd, aes(x = t, y = p)) +
  geom_area(fill = "gray", color = "black") +
  geom_ribbon(data = subset(dd, t > 1 & t < 2), aes(ymax = p), 
              ymin = 0, fill = "red", alpha = .75) +
  labs(x = "X", y = "Density", 
       title = "Distribution for X (time in seconds until particle decays)")
```

::: {.nonincremental}
- The probability that it takes between $1$ and $2$ seconds to decay is the area under the curve between $1$ and $2$. $P(1 < T < 2) = \color{red}{0.232}$
:::

------------------------------------------------------------------------

## Mean and Variance

Continuous variables have a **mean, variance, and standard deviation** too!

::: {.fragment}
::: {.nonincremental}
- We can't use the same definition as for discrete random variables.
$$E[X] = \sum_x xP[X=x]$$
:::
:::

::: {.fragment}
1. There are infinitely many values
2. With infinitely many, each specific one has probability 0
:::

::: {.fragment}
Instead, we have to use *calculus* to define mean and variance:
$$
\begin{align}
E[X] &= \int x f(x) \, dx\\
\mathrm{Var}(X) &= \int (x - \mu)^2 f(x) \, dx
\end{align}
$$
:::

------------------------------------------------------------------------

## Mean and Variance

$$
\begin{align}
E[X] &= \int x f(x) \, dx\\
\mathrm{Var}(X) &= \int (x - \mu)^2 f(x) \, dx\\
\mathrm{SD}(X) &=\sqrt{\mathrm{Var}(X)}
\end{align}
$$

::: {.fragment}
- These **integrals** are tools to meaningfully average infinitely many values
- *(We won't compute any integrals in this class!)*
:::

::: {.fragment}
As always...

- the **mean** of a random variable represents a **typical value**.
- the **standard deviation** represents the typical size of deviations from the mean.
:::



# The Normal Distribution


## The Normal Distribution

The **Normal distribution** is defined by two parameters:

1. Mean, $\mu$
2. Standard deviation, $\sigma$

::: {.fragment}
Suppose $X$ follows a Normal($\mu$,$\sigma$) distribution. The density function is
$$f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \cdot\exp \left(\frac{-(x-\mu)^2}{2\sigma^2}\right)$$
:::

```{r}
#| fig-height: 3
#| fig-width: 8
ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(-3, 3),
                geom = "area", fill = "steelblue") +
  scale_x_continuous(breaks = -1:1, 
                     labels = c(expression(mu - sigma), 
                               expression(mu), 
                               expression(mu + sigma))) +
  labs(x = "X", y = "Density", title = "The Normal Distribution")
```

------------------------------------------------------------------------

## Calculating Probabilities

`R` has built-in functions for calculating probabilities from a normal distribution.

::: {.fragment}
Suppose $X\sim \text{Normal}(\mu=75, sd=5)$. Then:
:::

::: {.fragment}
::: {.nonincremental}
- $P(X < 80) =$
```{r}
#| echo: true
pnorm(q = 80, mean = 75, sd = 5)
```
:::
:::

```{r}
#| fig-height: 3
s <- seq(55, 95, by = 0.1)
p <- dnorm(s, 75, 5)
dd <- data.frame(s, p)
dnorm3 <- function(x){dnorm(x, mean = 75, sd = 5)}

ggplot(dd, aes(x = s, y = p)) +
  geom_line(size = 0.5) +
  stat_function(fun = dnorm3, 
                xlim = c(55, 80),
                geom = "area", fill = "steelblue") +
  labs(x = element_blank(), y = "Density") +
  theme(panel.grid = element_blank()) +
  annotate(geom = "text", label = "Area = 0.84", x = 75, y = .04)
```

------------------------------------------------------------------------

## Calculating Probabilities

`R` has built-in functions for calculating probabilities from a normal distribution.

Suppose $X\sim \text{Normal}(\mu=75, sd=5)$. Then:

::: {.nonincremental}
- $P(X \geq 80) = P(X>80)=$ 
```{r}
#| echo: true
1-pnorm(q = 80, mean = 75, sd = 5)
```

:::

```{r}
#| fig-height: 3
s <- seq(55, 95, by = 0.1)
p <- dnorm(s, 75, 5)
dd <- data.frame(s, p)
dnorm3 <- function(x){dnorm(x, mean = 75, sd = 5)}

ggplot(dd, aes(x = s, y = p)) +
  geom_line(size = 0.5) +
  stat_function(fun = dnorm3, 
                xlim = c(80, 95),
                geom = "area", fill = "steelblue") +
  labs(x = element_blank(), y = "Density") +
  theme(panel.grid = element_blank()) +
  annotate(geom = "text", label = "Area = 0.16", x = 87, y = .03)
```

------------------------------------------------------------------------

## Calculating Probabilities

`R` has built-in functions for calculating probabilities from a normal distribution.

Suppose $X\sim \text{Normal}(\mu=75, sd=5)$. Then:

::: {.nonincremental}
- $P(70 \leq X \leq 80) = P(X\leq 80) - P(X\leq 70) =$

```{r}
#| echo: true
pnorm(q = 80, mean = 75, sd = 5) - pnorm(q = 70, mean = 75, sd = 5)
```
:::

```{r}
#| fig-height: 3
s <- seq(55, 95, by = 0.1)
p <- dnorm(s, 75, 5)
dd <- data.frame(s, p)
dnorm3 <- function(x){dnorm(x, mean = 75, sd = 5)}

ggplot(dd, aes(x = s, y = p)) +
  geom_line(size = 0.5) +
  stat_function(fun = dnorm3, 
                xlim = c(70, 80),
                geom = "area", fill = "steelblue") +
  labs(x = element_blank(), y = "Density") +
  theme(panel.grid = element_blank()) +
  annotate(geom = "text", label = "Area = 0.68", x = 75, y = .04)
```

------------------------------------------------------------------------

## Finding Quantiles

We can also use `R` to find **quantiles** of a Normal distribution.

::: {.fragment}
Suppose $X\sim \text{Normal}(\mu=75, sd=5)$. Then:
:::

::: {.fragment}
::: {.nonincremental}
- The 0.95 quantile is the value $c$ such that $P(X<c) = 0.95$

```{r}
#| echo: true
qnorm(p = 0.95, mean = 75, sd = 5)
```

:::
:::

```{r}
#| fig-height: 3
s <- seq(55, 95, by = 0.1)
p <- dnorm(s, 75, 5)
dd <- data.frame(s, p)
dnorm3 <- function(x){dnorm(x, mean = 75, sd = 5)}

ggplot(dd, aes(x = s, y = p)) +
  geom_line(size = 0.5) +
  stat_function(fun = dnorm3, 
                xlim = c(55, 83.224),
                geom = "area", fill = "steelblue") +
  scale_x_continuous(breaks = c(60, 70, 80, 83.22, 90),
                    labels = c(60, 70, 80, "c=83.22", 90)) +
  labs(x = element_blank(), y = "Density") +
  theme(panel.grid = element_blank()) +
  annotate(geom = "text", label = "Area = 0.95", x = 75, y = .04)
```

------------------------------------------------------------------------

## Scale and Translation Invariance

Suppose $X\sim\text{Normal}(\mu=0,\sigma=1)$ and $Y\sim\text{Normal}(\mu=2,\sigma=0.25)$.

```{r}
#| fig-height: 4
#| fig-width: 8
dnorm3 <- function(x){
  dnorm(x, mean = 2, sd = .5)
}
ggplot(data.frame(x = c(-3, 4)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(-3, 4),
                geom = "area", fill = "steelblue", alpha = 0.5) +
  stat_function(fun = dnorm3) + 
  stat_function(fun = dnorm3, 
                xlim = c(-3, 4),
                geom = "area", fill = "red", alpha = 0.5) +
  labs(x = "X", y = "Probability", title = "The Normal Distribution") +
  annotate(geom = "text", label = "X", x = 0, y = .15) +
  annotate(geom = "text", label = "Y", x = 2, y = .35)
```

::: {.fragment}
$X$ and $Y$ have different means, heights, and widths...

- But the **same shapes!**
:::

------------------------------------------------------------------------

## Scale and Translation Invariance

Suppose $X\sim\text{Normal}(\mu=0,\sigma=1)$ and $Y\sim\text{Normal}(\mu=2,\sigma=0.25)$.

```{r}
#| fig-height: 4
#| fig-width: 8
dnorm3 <- function(x){
  dnorm(x, mean = 2, sd = .5)
}
ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm) + 
  stat_function(fun = dnorm, 
                xlim = c(-3, 3),
                geom = "area", fill = "steelblue", alpha = 0.5) +
  labs(x = "X", y = "Probability", 
       title = "The Normal Distribution of X (I only changed the plot window)") +
  annotate(geom = "text", label = "X", x = 0, y = .15)
```

$X$ and $Y$ have different means, heights, and widths...

::: {.nonincremental}
- But the **same shapes!**
:::

------------------------------------------------------------------------

## Scale and Translation Invariance

Suppose $X\sim\text{Normal}(\mu=0,\sigma=1)$ and $Y\sim\text{Normal}(\mu=2,\sigma=0.25)$.

```{r}
#| fig-height: 4
#| fig-width: 8
dnorm3 <- function(x){
  dnorm(x, mean = 2, sd = .5)
}
ggplot(data.frame(x = c(.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm3) + 
  stat_function(fun = dnorm3, 
                xlim = c(.5, 3.5),
                geom = "area", fill = "red", alpha = 0.5) +
  labs(x = "X", y = "Probability", 
       title = "The Normal Distribution of Y (I only changed the plot window)") +
  annotate(geom = "text", label = "Y", x = 2, y = .3)
```

$X$ and $Y$ have different means, heights, and widths...

::: {.nonincremental}
- But the **same shapes!**
:::

------------------------------------------------------------------------

## Standardization

::: {.callout-note icon=false}
## Theorem: Standardization
Suppose $X\sim\text{Normal}(\mu,\sigma)$. Then, $Z = \frac{X - \mu}{\sigma}$ is a Normal random variable with mean 0 and standard deviation 1.
:::

::: {.fragment}
**Standard Normal:** a Normal random variable with mean $0$ and standard deviation $1$.

::: {.nonincremental}
- We call the process of subtracting off $\mu$ and dividing by $\sigma$, **standardizing.**
:::
:::

::: {.fragment}
::: {.nonincremental}
- Useful Fact: If $X\sim\text{Normal}(\mu=100,\sigma=10)$ and $Z\sim\text{Normal}(\mu=0,\sigma=1)$, then,
:::
:::

::: {.fragment}
$$P\Big[X < 90\Big] = P\Big[X<\text{ 1 SD below }\mu\Big] = P\Big[Z<-1\Big]$$
:::


# t distribution

------------------------------------------------------------------------

### t distribution

$X \sim$ t(df)

::: columns
::: column
Distribution:

$$
p(x) = \frac{\Gamma(\mbox{df} + 1)}{\sqrt{\mbox{df}\pi} \Gamma(2^{-1}\mbox{df})}\left(1 + \frac{x^2}{\mbox{df}} \right)^{-\frac{df + 1}{2}}
$$

where $-\infty < x < \infty$

-   Mean: 0

-   Standard deviation: $\sqrt{\mbox{df}/(\mbox{df} - 2)}$
:::

::: column
```{r, echo = FALSE}
library(tidyverse)
dat <- data.frame(x = seq(from = -5, to = 5, length.out = 1000)) %>%
  mutate(X1 = dnorm(x, mean = 0, sd = 1),
         X2 = dt(x, df = 2),
         X3 = dt(x, df = 8)) %>%
  pivot_longer(-x, names_to = "variable") %>%
  mutate(variable = case_when(
    variable == "X1" ~ "N(0, 1)",
    variable == "X2" ~ "t(2)",
    TRUE ~ "t(8)"
  ))
ggplot(dat, aes(x = x, y = value, color = variable)) + 
  geom_line(size = 1) +
  labs(title = "Standard Normal versus t",
       x = "x", y = "p(x)")
```
:::
:::

# It is time to recast some of the sample statistics we have been exploring as random variables!

------------------------------------------------------------------------

### Sample Statistics as Random Variables

Here are some of the sample statistics we've seen lately:

::: nonincremental
-   $\hat{p}$ = sample proportion of correct receiver guesses out of 329 trials

-   $\bar{x}_I - \bar{x}_N$ = difference in sample mean tuition between Ivies and non-Ivies

-   $\hat{p}_D - \hat{p}_Y$ = difference in sample improvement proportions between those who swam with dolphins and those who did not
:::

Why are these all random variables?

-   But they aren't **Bernoulli** random variables, nor **Binomial** random variables, nor **Normal** random variables, nor **t** random variables.

::: fragment
> *"All models are wrong but some are useful."* -- George Box
:::

------------------------------------------------------------------------

## Approximating These Distributions

-   $\hat{p}$ = sample proportion of correct receiver guesses out of 329 ESP trials

::: columns
::: {.column width=45%}
::: fragment

We generated its Null Distribution:

```{r, echo = FALSE}
library(infer)
# Construct data frame of sample results
esp <- data.frame(guess = c(rep("correct", 106),
                            rep("incorrect", 329 - 106)))

# Generate null distribution 
set.seed(4111)
null_dist <- esp %>%
  specify(response = guess, success = "correct") %>%
  hypothesize(null = "point", p = 0.25) %>%
  generate(reps = 1000, type = "draw") %>%
  calculate(stat ="prop")

# Graph null distribution with test statistic
dat <- data.frame(x = seq(16, 36, length.out = 1000)/100) %>%
  mutate(y = dnorm(x, mean = .25, sd = sqrt(.25*.75/329)))
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23) +
  xlim(.16, .36) + ylim(0, 17)
```
:::
:::

::: {.column}
::: fragment
Which is well approximated by the distribution of a N(0.25, 0.024).

```{r, echo = FALSE}
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23)  +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2) +
  xlim(.16, .36) + ylim(0, 17)
```
:::
:::
:::

------------------------------------------------------------------------

## Approximating These Distributions

-   $\bar{x}_I - \bar{x}_N$ = difference in sample mean tuition between Ivies and non-Ivies

::: columns
::: column
::: fragment
We generated its Null Distribution:

```{r, echo = FALSE}
#load the data
load("data/colleges_endow.Rdata")

#create ivy_binary
colleges <- colleges %>% 
  mutate(ivy_binary = case_when(
    tier_name == "Ivy Plus" ~ "Yes",
    tier_name != "Ivy Plus" ~ "No",
  ))

# Generate null distribution 
set.seed(4111)
null_dist <- colleges %>% 
  specify(scorecard_netprice_2013 ~ ivy_binary) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("Yes", "No"))

# Graph null distribution with test statistic
# Find sd
dat <- data.frame(x = seq(-6000, 6000, length.out = 1000)) %>%
  mutate(y = dnorm(x, mean = 0, sd = (8395/t.test(scorecard_netprice_2013 ~ ivy_binary, data = colleges)$statistic)))
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 21) +
  xlim(-6000, 6000) + ylim(0, .00024)
```
:::
:::

::: column
::: fragment
Which is somewhat approximated by the distribution of a N(0, 1036).

```{r, echo = FALSE}
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 21) +
  xlim(-6000, 6000) + ylim(0, .0004)  +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2)
```
:::
:::
:::

::: fragment
We will learn that a **standardized** version of the difference in sample means is **better** approximated by the distribution of a t(df = 7).
:::

------------------------------------------------------------------------

## Approximating These Distributions

-   $\hat{p}_D - \hat{p}_Y$ = difference in sample improvement proportions between those who swam with dolphins and those who did not

::: columns
::: column
::: fragment
We generated its Null Distribution:

```{r, echo = FALSE}
dolphins <- read.csv("data/Dolphins.txt", sep="")

null_dist <- dolphins %>%
  specify(improve ~ group, success = "yes") %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in props", order = c("Treatment", "Control"))

# Graph null distribution with test statistic
phat1 <- 10/15
phat2 <- 3/15
dat <- data.frame(x = seq(-65, 65, length.out = 1000)/100) %>%
  mutate(y = dnorm(x, mean = 0, sd = sqrt(phat1*(1-phat1)/15 + phat2*(1-phat2)/15)))
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23) +
  xlim(-.5, .5) + ylim(0, 7)
```
:::
:::

::: column
::: fragment
Which is **kinda somewhat** approximated by the probability function of a N(0, 0.16).

```{r, echo = FALSE}
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23)  +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2) +
  xlim(-.5, .5) + ylim(0, 7)
```
:::
:::
:::

------------------------------------------------------------------------

## Approximating These Distributions

-   How do I know **which** probability function is a good approximation for my sample statistic's distribution?

-   Once I have figured out a probability function that approximates the distribution of my sample statistic, how do I **use it** to do statistical inference?


# The Central Limit Theorem

------------------------------------------------------------------------

### Approximating Sampling Distributions

**Central Limit Theorem (CLT):** For random samples and a large sample size $(n)$, the sampling distribution of many sample statistics is approximately normal.

**Example**: Japanese Flowering Cherry Trees at Portland's Waterfront Park

```{r, echo = FALSE}
set.seed(18)
library(tidyverse)
library(pdxTrees)
waterfrontTrees <- get_pdxTrees_parks(park = "Gov Tom McCall Waterfront Park")
waterfrontTrees <- waterfrontTrees %>%
  mutate(Japanese_Flowering_Cherry = case_when(
    Common_Name == "Japanese Flowering Cherry" ~ "yes",
    Common_Name != "Japanese Flowering Cherry" ~ "no"
  ))

# Population distribution
p1 <- ggplot(data = waterfrontTrees, mapping = aes(x = Japanese_Flowering_Cherry)) +
  geom_bar(aes(y = ..prop.., group = 1), stat = "count")  +
  labs(title = "The Population\n Distribution")

# Construct the sampling distribution
samp_dist <- waterfrontTrees %>% 
  rep_sample_n(size = 100, reps = 1000) %>%
  group_by(replicate) %>%
  summarize(statistic = mean(Japanese_Flowering_Cherry == "yes"))

# Theoretical Distribution
dat <- data.frame(x = seq(10, 40, length.out = 1000)/100) %>%
  mutate(y = dnorm(x, mean = 0.231, sd = sqrt(0.231*(1 - 0.231)/100)))

# Graph the sampling distribution
p2 <- ggplot(data = samp_dist) + geom_blank() + theme_minimal()

library(cowplot)
plot_grid(p1, p2)
```

------------------------------------------------------------------------

### Approximating Sampling Distributions

**Central Limit Theorem (CLT):** For random samples and a large sample size $(n)$, the sampling distribution of many sample statistics is approximately normal.

**Example**: Japanese Flowering Cherry Trees at Portland's Waterfront Park

```{r, echo = FALSE}
# Graph the sampling distribution
p2 <- ggplot(data = samp_dist, mapping = , aes(x = statistic, y = ..density..)) +
  geom_histogram(bins = 31) +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2) + labs(title = "The Sampling\n Distribution")

library(cowplot)
plot_grid(p1, p2)
```

-   But **which** Normal? (What is the value of $\mu$ and $\sigma$?)

------------------------------------------------------------------------

### Approximating Sampling Distributions

**Question**: But **which** normal? (What is the value of $\mu$ and $\sigma$?)

-   The sampling distribution of a statistic is always centered around: _____

-   The CLT also provides formula estimates of the standard error.

    -   The formula varies based on the statistic.

------------------------------------------------------------------------

### Approximating the Sampling Distribution of a Sample Proportion

CLT says: For large $n$ (At least 10 successes and 10 failures),

$$
\hat{p} \sim N \left(p, \sqrt{\frac{p(1-p)}{n}} \right)
$$

**Example**: Japanese Flowering Cherry Trees at Portland's Waterfront Park

-   Parameter: $p$ = proportion of Japanese Flowering Cherry Trees at Portland's Waterfront Park = 0.231

-   Statistic: $\hat{p}$ = proportion of Japanese Flowering Cherries in a sample of 100 trees

::: fragment
$$
\hat{p} \sim N \left(0.231, \sqrt{\frac{0.231(1-0.231)}{100}} \right)
$$

**NOTE**: Can plug in the true parameter here because we had data on the whole population.
:::

------------------------------------------------------------------------

### Approximating the Sampling Distribution of a Sample Proportion

**Question**: What do we do when we don't have access to the whole population?

Have:

$$
\hat{p} \sim N \left(p, \sqrt{\frac{p(1-p)}{n}} \right)
$$

::: fragment
**Answer**: We will have to estimate the SE.
:::

------------------------------------------------------------------------

### Approximating the Sampling Distribution of a Sample Mean

There is a version of the CLT for many of our sample statistics.

For the sample mean, the CLT says:

::: {.callout-note icon=false}
## Theorem: Central Limit Theorem

For large $n$ (At least 30 observations),

$$
\bar{x} \sim N \left(\mu, \frac{\sigma}{\sqrt{n}} \right)
$$

:::

# Next week: We'll use the approximate distribution of the sample statistic (given by the CLT) to construct confidence intervals and to conduct hypothesis tests!